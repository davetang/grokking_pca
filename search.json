[
  {
    "objectID": "notebook/07-chap.html",
    "href": "notebook/07-chap.html",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "",
    "text": "Install required packages if missing.\n\ncran_packages &lt;- c(\"ggplot2\", \"factoextra\", \"ade4\", \"pheatmap\", \"MASS\", \"tibble\")\nbioc_packages &lt;- c(\"SummarizedExperiment\", \"phyloseq\", \"airway\", \"Hiiragi2013\", \"Biobase\")\n\ncran_missing &lt;- cran_packages[!sapply(cran_packages, requireNamespace, quietly = TRUE)]\nif (length(cran_missing) &gt; 0) {\n  install.packages(cran_missing)\n}\n\nbioc_missing &lt;- bioc_packages[!sapply(bioc_packages, requireNamespace, quietly = TRUE)]\nif (length(bioc_missing) &gt; 0) {\n  if (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n    install.packages(\"BiocManager\")\n  }\n  BiocManager::install(bioc_missing)\n}"
  },
  {
    "objectID": "notebook/07-chap.html#setup",
    "href": "notebook/07-chap.html#setup",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "",
    "text": "Install required packages if missing.\n\ncran_packages &lt;- c(\"ggplot2\", \"factoextra\", \"ade4\", \"pheatmap\", \"MASS\", \"tibble\")\nbioc_packages &lt;- c(\"SummarizedExperiment\", \"phyloseq\", \"airway\", \"Hiiragi2013\", \"Biobase\")\n\ncran_missing &lt;- cran_packages[!sapply(cran_packages, requireNamespace, quietly = TRUE)]\nif (length(cran_missing) &gt; 0) {\n  install.packages(cran_missing)\n}\n\nbioc_missing &lt;- bioc_packages[!sapply(bioc_packages, requireNamespace, quietly = TRUE)]\nif (length(bioc_missing) &gt; 0) {\n  if (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n    install.packages(\"BiocManager\")\n  }\n  BiocManager::install(bioc_missing)\n}"
  },
  {
    "objectID": "notebook/07-chap.html#learning-outcomes",
    "href": "notebook/07-chap.html#learning-outcomes",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nSee examples of matrices that come up in the study of biological data.\nPerform dimension reduction to understand correlations between variables.\nPreprocess, rescale, and centre data before starting a multivariate analysis.\nBuild new variables, called principal components (PCs), that are more useful than the original measurements.\nSee what is “under the hood” of PCA: the singular value decomposition of a matrix.\nVisualise what SVD achieves and learn how to choose the number of principal components.\nRun through a complete PCA from start to finish.\nProject factor covariates onto the PCA map for a more useful interpretation of results."
  },
  {
    "objectID": "notebook/07-chap.html#example-data",
    "href": "notebook/07-chap.html#example-data",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "Example data",
    "text": "Example data\nThe turtles dataset is a matrix of three dimensions of biometric measurements on painted turtles.\n\nturtles = read.table(\"../data/PaintedTurtles.txt\", header = TRUE)\nturtles[1:4, ]\n\n  sex length width height\n1   f     98    81     38\n2   f    103    84     38\n3   f    103    86     42\n4   f    105    86     40\n\n\nThe athletes dataset contains the performance of 33 athletes in the 10 disciplines of the decathlon.\n\nload(\"../data/athletes.RData\")\nathletes[1:3, ]\n\n   m100 long weight highj  m400  m110  disc pole javel  m1500\n1 11.25 7.43  15.48  2.27 48.90 15.13 49.28  4.7 61.32 268.95\n2 10.87 7.45  14.97  1.97 47.71 14.46 44.36  5.1 61.76 273.02\n3 11.18 7.44  14.20  1.97 48.29 14.81 43.66  5.2 64.16 263.20\n\n\nSummary of each decathlon event.\n\nstopifnot(nrow(athletes) == 33, ncol(athletes) == 10)\nathletesSummary = lapply(athletes, function(x) c(min = min(x), max = max(x), mean = mean(x), sd = sd(x)))\nathletesSummary\n\n$m100\n      min       max      mean        sd \n10.620000 11.570000 11.196364  0.243321 \n\n$long\n      min       max      mean        sd \n6.2200000 7.7200000 7.1333333 0.3043401 \n\n$weight\n      min       max      mean        sd \n10.270000 16.600000 13.976364  1.331991 \n\n$highj\n      min       max      mean        sd \n1.7900000 2.2700000 1.9827273 0.0939838 \n\n$m400\n     min      max     mean       sd \n47.44000 51.28000 49.27667  1.06966 \n\n$m110\n       min        max       mean         sd \n14.1800000 16.2000000 15.0487879  0.5067652 \n\n$disc\n      min       max      mean        sd \n34.360000 50.660000 42.353939  3.719131 \n\n$pole\n      min       max      mean        sd \n4.0000000 5.7000000 4.7393939 0.3344206 \n\n$javel\n      min       max      mean        sd \n49.520000 72.600000 59.438788  5.495998 \n\n$m1500\n     min      max     mean       sd \n256.6400 303.1700 276.0385  13.6571 \n\n\nThe Msig3transp dataset contains gene expression profiles of sorted T-cell populations from different subjects. The columns are a subset of gene expression measurements and correspond to 156 genes that show differential expression between cell types.\n\nload(\"../data/Msig3transp.RData\")\ndim(Msig3transp)\n\n[1]  30 156\n\n\nThe GlobalPatterns dataset is from the phyloseq package. It contains counts of different species (or Operational Taxonomic Units, OTUs) of bacteria, which are identified by numerical tags. The rows are labelled according to the samples in which they were measured and the (integer) numbers represent the number of times each OTU was observed in each of the samples.\n\ndata(\"GlobalPatterns\", package = \"phyloseq\")\nGPOTUs = as.matrix(t(phyloseq::otu_table(GlobalPatterns)))\nGPOTUs[1:4, 6:13]\n\nOTU Table:          [4 taxa and 8 samples]\n                     taxa are rows\n        246140 143239 244960 255340 144887 141782 215972 31759\nCL3          0      7      0    153      3      9      0     0\nCC1          0      1      0    194      5     35      3     1\nSV1          0      0      0      0      0      0      0     0\nM31Fcsw      0      0      0      0      0      0      0     0\n\n\nThe airway dataset contains RNA-seq data for different biological samples (columns).\n\nlibrary(\"SummarizedExperiment\")\n\nLoading required package: MatrixGenerics\n\n\nLoading required package: matrixStats\n\n\n\nAttaching package: 'MatrixGenerics'\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n    colWeightedMeans, colWeightedMedians, colWeightedSds,\n    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n    rowWeightedSds, rowWeightedVars\n\n\nLoading required package: GenomicRanges\n\n\nLoading required package: stats4\n\n\nLoading required package: BiocGenerics\n\n\nLoading required package: generics\n\n\n\nAttaching package: 'generics'\n\n\nThe following objects are masked from 'package:base':\n\n    as.difftime, as.factor, as.ordered, intersect, is.element, setdiff,\n    setequal, union\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, is.unsorted, lapply, Map, mapply, match, mget,\n    order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n    rbind, Reduce, rownames, sapply, saveRDS, table, tapply, unique,\n    unsplit, which.max, which.min\n\n\nLoading required package: S4Vectors\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following object is masked from 'package:utils':\n\n    findMatches\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\nLoading required package: Seqinfo\n\n\nLoading required package: Biobase\n\n\nWelcome to Bioconductor\n\n    Vignettes contain introductory material; view with\n    'browseVignettes()'. To cite Bioconductor, see\n    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\n\n\nAttaching package: 'Biobase'\n\n\nThe following object is masked from 'package:MatrixGenerics':\n\n    rowMedians\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    anyMissing, rowMedians\n\ndata(\"airway\", package = \"airway\")\nassay(airway)[1:3, 1:4]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513\nENSG00000000003        679        448        873        408\nENSG00000000005          0          0          0          0\nENSG00000000419        467        515        621        365\n\n\nThe metab dataset contains aligned mass spectroscopy peaks or molecules identified through their m / z ratios; the entries in the matrix are the measured intensities.\n\nmetab = t(as.matrix(read.csv(\"../data/metabolites.csv\", row.names = 1)))\nmetab[1:4, 1:4]\n\n         146.0985388 148.7053275 310.1505057 132.4512963\nKOGCHUM1    29932.36    17055.70     1132.82    785.5129\nKOGCHUM2    94067.61    74631.69    28240.85   5232.0499\nKOGCHUM3   146411.33   147788.71    64950.49  10283.0037\nWTGCHUM1   229912.57   384932.56   220730.39  26115.2007\n\n\nFrequency of zeros.\n\nprop.table(table(as.vector(GPOTUs == 0)))\n\n\n    FALSE      TRUE \n0.2093168 0.7906832 \n\nprop.table(table(metab == 0))\n\n\n    FALSE      TRUE \n0.8966461 0.1033539 \n\nprop.table(table(assay(airway) == 0))\n\n\n    FALSE      TRUE \n0.3889591 0.6110409"
  },
  {
    "objectID": "notebook/07-chap.html#low-dimensional-data-summaries",
    "href": "notebook/07-chap.html#low-dimensional-data-summaries",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "Low-dimensional data summaries",
    "text": "Low-dimensional data summaries\nIf we are studying only one variable, e.g., just the third column of the turtles matrix, we say we are looking at one-dimensional data. Such a vector, say all the turtle weights, can be visualised using a histogram. If we compute a one-number summary, say mean or median, we have made a zero-dimensional summary of our one-dimensional data. This is already an example of dimension reduction.\nWhen considering two variables (x and y) measured together on a set of observations, the correlation coefficient measures how the variables co-vary. This is a single-number summary of two-dimensional data. Its formula involves the summaries \\(\\bar{x}\\) and \\(\\bar{y}\\):\n\\[\n\\hat{p} = \\frac{\\sum^n_{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum^n_{i=1}(x_i - \\bar{x})^2} \\sqrt{\\sum^n_{j=1}(y_j - \\bar{y})^2}}\n\\]\nIn R, the cor function calculates the correlation coefficient and when applied to a matrix, this function computes all the two-way correlations between continuous variables.\n\ncor(turtles[, -1])\n\n          length     width    height\nlength 1.0000000 0.9783116 0.9646946\nwidth  0.9783116 1.0000000 0.9605705\nheight 0.9646946 0.9605705 1.0000000\n\n\nAll the values are close to 1 and it seems that all three variables mostly reflect the same “underlying” variable, which we might interpret as the size of the turtle."
  },
  {
    "objectID": "notebook/07-chap.html#preprocessing-the-data",
    "href": "notebook/07-chap.html#preprocessing-the-data",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "Preprocessing the data",
    "text": "Preprocessing the data\nIn many cases, different variables are measured in different units, so they have different baselines and different scales. These are not directly comparable in their original form.\nFor PCA and many other methods, we therefore need to transform the numeric values to some common scale in order to make comparisons meaningful.\n\nCentering means subtracting the mean, so that the mean of the centered data is at the origin.\nScaling or standardising means dividing by the standard deviation, so that the new standard deviation is 1.\n\nThe correlation coefficient is simply the vector product of the centered and scaled variables. To perform these operations, there is the scale function, whose default behaviour when given a matrix or data frame is to make every column have a mean of 0 and a standard deviation of 1.\nStandard deviation and mean before scaling.\n\napply(turtles[,-1], 2, sd)\n\n   length     width    height \n20.481602 12.675838  8.392837 \n\napply(turtles[,-1], 2, mean)\n\n   length     width    height \n124.68750  95.43750  46.33333 \n\n\nStandard deviation and mean after scaling.\n\nscaledTurtles = scale(turtles[, -1])\napply(scaledTurtles, 2, mean)\n\n       length         width        height \n-1.432050e-18  1.940383e-17 -2.870967e-16 \n\napply(scaledTurtles, 2, sd)\n\nlength  width height \n     1      1      1 \n\n\nPlot scaled data.\n\nlibrary(ggplot2)\ndata.frame(scaledTurtles, sex = turtles[, 1]) |&gt;\n  ggplot(aes(x = width, y = height, group = sex)) +\n    geom_point(aes(color = sex)) + coord_fixed()\n\n\n\n\n\n\n\n\nThe aim of transformations is (usually) variance stabilisation, i.e., to make the variances of replicate measurements of one and the same variable in different parts of its dynamic range more similar. The standardising transformation using scale aims to make the scale (as measured by mean and standard deviation) of different variables the same.\nSometimes it is preferable to leave variables at different scales because they are truly of different importance. If their original scale is relevant, then we can (and should) leave the data alone. After pre-processing the data, we are ready to undertake data simplification through dimension reduction."
  },
  {
    "objectID": "notebook/07-chap.html#dimension-reduction",
    "href": "notebook/07-chap.html#dimension-reduction",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "Dimension reduction",
    "text": "Dimension reduction\nDimension reduction was invented in 1901 by Karl Pearson as a way of reducing a two-variable scatterplot to a single coordinate. It was used by statisticians in the 1930s to summarise a battery of psychological tests run on the same subjects, thus providing overall scores that summarise many test variables at once. This idea of principal scores inspired the name principal component analysis (PCA).\nPCA is called an unsupervised learning technique because, as in clustering, it treats all variables as having the same status. We are not trying to predict or explain one particular variable’s value from the others; rather, we are trying to find a mathematical model for an underlying structure for all the variables.\nPCA is primarily an exploratory technique that produces maps that show the relations between variables and between observations in a useful way.\nWe use geometric projections that take points in higher-dimensional spaces and project them down onto lower dimensions. In the example below, point A is projected onto the red line generated by the vector \\(v\\). The dashed projection line is perpendicular (or orthogonal) to the red line. The intersection point of the projection line and the red line is called the orthogonal projection of A onto the red line generated by the vector \\(v\\).\n\nx1=1;x2=3;y1=1;y2=2;ax=2.5;ay=3;\ndf=data.frame(x=c(x1,x2,ax),y=c(y1,y2,ay))\nggplot(df, aes(x=x,y=y)) + \n       geom_point(size=2) +\n       geom_abline(intercept=0.5,slope=0.5, color=\"red\", linewidth=1.3) + \n       xlim(c(0,4)) + \n       ylim(c(0,4)) +\n       geom_segment(x=x1,y=y1,xend=x2-0.5,yend=y2-0.25,arrow=arrow(length = unit(0.3,\"cm\")),color=\"blue\") +\n       geom_segment(x=ax,y=ay,xend=x2,yend=y2,arrow=arrow(length = unit(0.3,\"cm\")),color=\"orange\",\n                    linetype = 5, linewidth = 1.2, alpha = 0.5) + \n       annotate(\"text\", x = ax+0.2, y = ay+0.15, label = \"A\", size=6) +\n       annotate(\"text\", x = x2, y = y2-0.5, label = \"proj_v(A)\", size=6) +\n       annotate(\"text\", x = x1+0.75, y = y1+0.24, label = \"v\", size=6, color=\"blue\") +\n       annotate(\"text\", x = x1-0.2, y = y1+ 0.2, label = \"O\", size=6) +\n       coord_fixed() + \n       theme_void() +\n       geom_point(size=2)\n\n\n\n\n\n\n\n\nPCA is a linear technique, meaning that we look for linear relations between variables and that we will use new variables that are linear functions of the original ones (\\(f(ax + by) = af(x) + b(y)\\)). The linearity constraint makes computations particularly easy.\n\nLower-dimensional projections\nThe code below shows one way of projecting two-dimensional data onto a line.\n\nload(\"../data/athletes.RData\")\nathletes = data.frame(scale(athletes))\nath_gg = ggplot(athletes, aes(x = weight, y = disc)) +\n  geom_point(size = 2, shape = 21)\nath_gg + geom_point(aes(y = 0), colour = \"red\") +\n  geom_segment(aes(xend = weight, yend = 0), linetype = \"dashed\")\n\n\n\n\n\n\n\n\nThe scatterplot shows two variables (weight and disc) that is projected onto the horizontal x-axis (defined by \\(y = 0\\)) in red, and the lines of projection appear as dashed.\n\n\nHow do we summarise two-dimensional data by a line?\nIn general, we lose information about the points when we project from two dimensions (a plane) onto one (a line). If we do it just by using the original coordinates, as we did on the weight variable above, we lose all the information about the disc variable.\nOur goal is to keep as much information as we can about both variables. There are actually many ways of projecting a point cloud onto a line. One is to use what are known as regressionlines.\n\nRegressing one variable on the other\nLinear regression is a supervised method that gives preference to minimising the residual sum of squares in one direction: that of the response variable.\nBelow we use the lm (linear model) function to find the regression line. Its slope and intercept are given by the values in the coefficients slot of the resulting object reg1.\n\nreg1 = lm(disc ~ weight, data = athletes)\na1 = reg1$coefficients[1] # intercept\nb1 = reg1$coefficients[2] # slope\npline1 = ath_gg + geom_abline(intercept = a1, slope = b1,\n    col = \"blue\", linewidth = 1.5)\npline1 + geom_segment(aes(xend = weight, yend = reg1$fitted),\n    colour = \"red\", arrow = arrow(length = unit(0.15, \"cm\")))\n\n\n\n\n\n\n\n\nThe blue line minimises the sum of squares of the vertical residuals (in red).\nThe code below shows the line produced when the roles of the two variables are reversed; weight becomes the response variable.\n\nreg2 = lm(weight ~ disc, data = athletes)\na2 = reg2$coefficients[1] # intercept\nb2 = reg2$coefficients[2] # slope\npline2 = ath_gg + geom_abline(intercept = -a2/b2, slope = 1/b2,\n    col = \"darkgreen\", linewidth = 1.5)\npline2 + geom_segment(aes(xend=reg2$fitted, yend=disc),\n    colour = \"orange\", arrow = arrow(length = unit(0.15, \"cm\")))\n\n\n\n\n\n\n\n\nThe green line minimises the sum of squares of the horizontal residuals (in orange).\nEach of the regression lines gives us an approximate linear relationship between disc and weight. However, the relationship differs depending on which variable we choose to be the predictor and which the response.\nHow large is the variance of the projected points that lie on the blue regression line? Compare this to the variance of the data when projected on the original axes, weight and disc.\nPythagoras’ theorem tells us that the square of the hypotenuse of a right-angled triangle is equal to the sum of the squares of the other two sides, which we apply as follows.\n\nvar(athletes$weight) + var(reg1$fitted)\n\n[1] 1.650204\n\n\nThe variances of the points along the original axes weight and disc are 1, since we scaled the variables.\nVariance of the projected points that lie on the green regression line.\n\nvar(athletes$disc) + var(reg2$fitted)\n\n[1] 1.650204\n\n\n\n\nA line that minimises distance in both directions\nThe code below generates a line that minimises the sum of squares of the orthogonal (perpendicular) projections of data points onto it; we call this the principal component line.\n\nstopifnot(all.equal(c(var(athletes$weight), var(athletes$disc)), c(1,1)))\nxy = cbind(athletes$disc, athletes$weight)\nsvda = svd(xy)\npc = xy %*% svda$v[, 1] %*% t(svda$v[, 1])\nbp = svda$v[2, 1] / svda$v[1, 1]\nap = mean(pc[, 2]) - bp * mean(pc[, 1])\nath_gg + geom_segment(xend = pc[, 1], yend = pc[, 2]) +\n  geom_abline(intercept = ap, slope = bp, col = \"purple\", linewidth = 1.5)\n\n\n\n\n\n\n\n\nAll of the three ways of fitting a line are shown together in the plot below.\n\npline1 + geom_segment(aes(xend = weight, yend = reg1$fitted), colour = \"blue\", alpha = 0.35) +\n  geom_abline(intercept = -a2/b2, slope = 1/b2, col = \"darkgreen\", linewidth = 1.5, alpha = 0.8) +\n  geom_segment(aes(xend = reg2$fitted, yend = disc), colour = \"orange\", alpha = 0.35) +\n  geom_abline(intercept = ap, slope = bp, col = \"purple\", linewidth = 1.5, alpha = 0.8) +\n  geom_segment(xend = pc[, 1], yend = pc[, 2], colour = \"purple\", alpha = 0.35) + coord_fixed()\n\n\n\n\n\n\n\n\nThe purple PCA line cuts exactly in the middle of both regression lines. The blue line minimises the sum of squares of the vertical residuals, the green line minimises the horizontal residuals, and the purple line, called the principal component, minimises the orthogonal projections.\nThe variance of the points on the purple line can be calculated by using var on the new coordinates.\n\napply(pc, 2, var)\n\n[1] 0.9031761 0.9031761\n\nsum(apply(pc, 2, var))\n\n[1] 1.806352\n\n\nThe variance along the purple line is larger than the variance using the regression lines.\nPythagoras’ theorem tells us two interesting things here:\n\nIf we are minimising in both horizontal and vertical directions, we are in fact minimising the orthogonal projections onto the line from each point.\nThe total variability of the points is measured by the sum of squares of the projection of the points onto the center of gravity, which is the origin (0,0) if the data are centered. This is called the total variance or the inertia of the point cloud. This inertia can be decomposed into the sum of the squares of the projections onto the line plus the variances along that line. For a fixed variance, minimising the projection distances also maximises the variance along that line. Often we define the first principal component as the line with maximum variance."
  },
  {
    "objectID": "notebook/07-chap.html#the-new-linear-combinations",
    "href": "notebook/07-chap.html#the-new-linear-combinations",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "The new linear combinations",
    "text": "The new linear combinations\nThe PC line we found in the previous section could be written:\n\\[\nPC = \\frac{1}{2}disc + \\frac{1}{2}weight\n\\]\nPrincipal components are linear combinations of the variables that were originally measured: they provide a new coordinate system. To understand what a linear combination really is, we can take an analogy. When making a juice mix, you will follow a recipe like:\n\\[\nV = 2 \\times Beet + 1 \\times Carrot + \\frac{1}{2} Gala + \\frac{1}{2}GrannySmith + 0.02 \\times Ginger + 0.25 \\times lemon\n\\]\nThis recipe is a linear combination of individual juice types (the original variables). The result is a new variable, V, and the coefficients (2, 1, 0.5, 0.5, 0.02, 0.25) are called the loadings.\n\nOptimal lines\nA linear combination of variables defines a line in higher dimensions in the same way we constructed lines in the scatterplot plane of two dimensions. As we saw in that case, there are many ways to choose lines onto which we project the data; there is, however, a “best” line for our purpose.\nThe total variance of all the points in all the variables can be decomposed. In PCA, we use the fact that the total sums of squares of the distances between the points and any line can be decomposed into the distance to the line and the variance along the line.\nWe saw that the principal component mimimises the distance to the line, and it also maximises the variance of the projections along the line."
  },
  {
    "objectID": "notebook/07-chap.html#the-pca-workflow",
    "href": "notebook/07-chap.html#the-pca-workflow",
    "title": "Chapter 7: Multivariate Analysis",
    "section": "The PCA workflow",
    "text": "The PCA workflow\n\n.savedopt = options(digits = 3)\nX = matrix(c(780,  75, 540,\n             936,  90, 648,\n            1300, 125, 900,\n             728,  70, 504), nrow = 3)\nu = c(0.8196, 0.0788, 0.5674)\nv = c(0.4053, 0.4863, 0.6754, 0.3782)\ns1 = 2348.2\nsum(u^2)\n\n[1] 1\n\nsum(v^2)\n\n[1] 1\n\ns1 * u %*% t(v)\n\n     [,1] [,2] [,3] [,4]\n[1,]  780  936 1300  728\n[2,]   75   90  125   70\n[3,]  540  648  900  504\n\nX - s1 * u %*% t(v)\n\n         [,1]   [,2]   [,3]   [,4]\n[1,] -0.03419 0.0745 0.1355 0.1221\n[2,]  0.00403 0.0159 0.0252 0.0186\n[3,] -0.00903 0.0691 0.1182 0.0982\n\noptions(.savedopt)\n\n\nsvd(X)$u[, 1]\n\n[1] 0.81963482 0.07881104 0.56743949\n\nsvd(X)$v[, 1]\n\n[1] 0.4052574 0.4863089 0.6754290 0.3782403\n\nsum(svd(X)$u[, 1]^2)\n\n[1] 1\n\nsum(svd(X)$v[, 1]^2)\n\n[1] 1\n\nsvd(X)$d\n\n[1] 2.348244e+03 2.141733e-13 6.912584e-15\n\n\n\nXtwo = matrix(c(12.5, 35.0, 25.0, 25, 9, 14, 26, 18, 16, 21, 49, 32,\n       18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol = 4, byrow = TRUE)\nUSV = svd(Xtwo)\nnames(USV)\n\n[1] \"d\" \"u\" \"v\"\n\nUSV$d\n\n[1] 1.350624e+02 2.805191e+01 3.103005e-15 1.849559e-15\n\n\n\nXtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1])\n\n            [,1]       [,2]        [,3]        [,4]\n[1,]  0.87481760  19.045230 -10.1044650  1.74963521\n[2,]  0.08079747   1.759002  -0.9332405  0.16159494\n[3,] -0.04700978  -1.023427   0.5429803 -0.09401956\n[4,]  0.16159494   3.518005  -1.8664809  0.32318987\n[5,] -0.69632883 -15.159437   8.0428540 -1.39265765\n\nXtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -\n       USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2])\n\n             [,1]          [,2]         [,3]         [,4]\n[1,] 7.216450e-15 -1.065814e-14 8.881784e-15 4.884981e-15\n[2,] 2.040035e-15 -5.995204e-15 1.054712e-14 3.219647e-15\n[3,] 2.865763e-15 -9.547918e-15 1.554312e-15 6.231127e-15\n[4,] 4.385381e-15 -5.773160e-15 1.776357e-14 7.049916e-15\n[5,] 5.107026e-15 -1.776357e-15 1.776357e-14 1.776357e-14\n\n\n\nstopifnot(max(abs(\nXtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -\n       USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2]))) &lt; 1e-12,\nmax(abs(USV$d[3:4])) &lt; 1e-13)\n\n\nt(USV$u) %*% USV$u\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  1.000000e+00 -1.665335e-16  0.000000e+00 -8.326673e-17\n[2,] -1.665335e-16  1.000000e+00  1.665335e-16 -5.551115e-17\n[3,]  0.000000e+00  1.665335e-16  1.000000e+00 -5.551115e-17\n[4,] -8.326673e-17 -5.551115e-17 -5.551115e-17  1.000000e+00\n\nt(USV$v) %*% USV$v\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  1.000000e+00  8.326673e-17  1.387779e-17 -5.551115e-17\n[2,]  8.326673e-17  1.000000e+00 -3.642919e-17 -6.938894e-17\n[3,]  1.387779e-17 -3.642919e-17  1.000000e+00  2.775558e-17\n[4,] -5.551115e-17 -6.938894e-17  2.775558e-17  1.000000e+00\n\n\n\nturtles.svd = svd(scaledTurtles)\nturtles.svd$d\n\n[1] 11.746475  1.419035  1.003329\n\nturtles.svd$v\n\n          [,1]       [,2]        [,3]\n[1,] 0.5787981  0.3250273  0.74789704\n[2,] 0.5779840  0.4834699 -0.65741263\n[3,] 0.5752628 -0.8127817 -0.09197088\n\ndim(turtles.svd$u)\n\n[1] 48  3\n\n\n\nsum(turtles.svd$v[,1]^2)\n\n[1] 1\n\nsum(turtles.svd$d^2) / 47\n\n[1] 3\n\n\n\nstopifnot(max(abs(turtles.svd$v[,1]^2 - 1/3)) &lt; 0.01)\nUS = turtles.svd$u[,1, drop = FALSE] %*% turtles.svd$d[1]\nXV = scaledTurtles %*% turtles.svd$v[, 1, drop = FALSE]\nmax(abs(US-XV))\n\n[1] 2.88658e-15\n\nstopifnot(max(abs(US-XV)) &lt; 1e-9)\n\n\nsvda$v[,1]\n\n[1] -0.7071068 -0.7071068\n\n\n\nppdf = tibble::tibble(PC1n = -svda$u[, 1] * svda$d[1],\n              PC2n =  svda$u[, 2] * svda$d[2])\ngg = ggplot(ppdf, aes(x = PC1n, y = PC2n)) + \n    geom_point() + \n    geom_hline(yintercept = 0, color = \"purple\", linewidth = 1.5, alpha = 0.5) +\n    xlab(\"PC1 \")+ ylab(\"PC2\") + xlim(-3.5, 2.7) + ylim(-2, 2) + coord_fixed()\ngg + geom_point(aes(x = PC1n, y = 0), color = \"red\") +\n     geom_segment(aes(xend = PC1n, yend = 0), color = \"red\") \n\n\n\n\n\n\n\ngg + geom_point(aes(x = 0, y = PC2n), color = \"blue\") +\n     geom_segment(aes(yend = PC2n, xend = 0), color = \"blue\") +\n     geom_vline(xintercept = 0, color = \"skyblue\", linewidth = 1.5, alpha = 0.5) \n\n\n\n\n\n\n\n\n\nsum(ppdf$PC2n^2) \n\n[1] 6.196729\n\nsvda$d[2]^2\n\n[1] 6.196729\n\nstopifnot(abs(sum(ppdf$PC2n^2) - svda$d[2]^2)&lt;1e-9)\n\n\nmean(ppdf$PC2n) \n\n[1] 5.735068e-16\n\nvar(ppdf$PC2n) * (nrow(ppdf)-1)\n\n[1] 6.196729\n\nstopifnot(abs(var(ppdf$PC2n) * (nrow(ppdf)-1) - svda$d[2]^2) &lt; 1e-9, abs(mean(ppdf$PC2n)) &lt; 1e-9)\n\n\nvar(ppdf$PC1n) \n\n[1] 1.806352\n\nvar(ppdf$PC2n) \n\n[1] 0.1936478\n\nstopifnot(var(ppdf$PC1n) &gt; var(ppdf$PC2n))\n\n\nsd(ppdf$PC1n) / sd(ppdf$PC2n)\n\n[1] 3.054182\n\nsvda$d[1] / svda$d[2]\n\n[1] 3.054182\n\nstopifnot(sd(ppdf$PC1n) / sd(ppdf$PC2n) - svda$d[1] / svda$d[2] &lt; 1e-9)\n\n\ncor(scaledTurtles)\n\n          length     width    height\nlength 1.0000000 0.9783116 0.9646946\nwidth  0.9783116 1.0000000 0.9605705\nheight 0.9646946 0.9605705 1.0000000\n\npcaturtles = princomp(scaledTurtles)\npcaturtles\n\nCall:\nprincomp(x = scaledTurtles)\n\nStandard deviations:\n   Comp.1    Comp.2    Comp.3 \n1.6954576 0.2048201 0.1448180 \n\n 3  variables and  48 observations.\n\n\n\nlibrary(\"factoextra\")\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_eig(pcaturtles, geom = \"bar\", bar_width = 0.4) + ggtitle(\"\")\n\n\n\n\n\n\n\n\n\nsvd(scaledTurtles)$v[, 1]\nprcomp(turtles[, -1])$rotation[, 1]\nprincomp(scaledTurtles)$loadings[, 1]\nlibrary(\"ade4\")\ndudi.pca(turtles[, -1], nf = 2, scannf = FALSE)$c1[, 1]\n\n\nres = princomp(scaledTurtles)\nPC1 = scaledTurtles %*% res$loadings[,1]\nsd1 = sqrt(mean(res$scores[, 1]^2))\n\n\nlibrary(\"ade4\")\n\n\nAttaching package: 'ade4'\n\n\nThe following object is masked from 'package:GenomicRanges':\n\n    score\n\n\nThe following object is masked from 'package:BiocGenerics':\n\n    score\n\nlibrary(\"tibble\")\n\nfviz_pca_biplot(pcaturtles, label = \"var\", habillage = turtles[, 1]) +\n  ggtitle(\"\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\npcadudit = dudi.pca(scaledTurtles, nf = 2, scannf = FALSE)\napply(pcadudit$li, 2, function(x) sum(x^2)/48)\n\n     Axis1      Axis2 \n2.93573765 0.04284387 \n\npcadudit$eig\n\n[1] 2.93573765 0.04284387 0.02141848\n\nfviz_pca_var(pcaturtles, col.circle = \"black\") + ggtitle(\"\") +\n  xlim(c(-1.2, 1.2)) + ylim(c(-1.2, 1.2))\n\n\n\n\n\n\n\nsvd(scaledTurtles)$d/pcaturtles$sdev\n\n  Comp.1   Comp.2   Comp.3 \n6.928203 6.928203 6.928203 \n\nsqrt(47)\n\n[1] 6.855655\n\ncor(athletes) |&gt; round(1)\n\n       m100 long weight highj m400 m110 disc pole javel m1500\nm100    1.0 -0.5   -0.2  -0.1  0.6  0.6  0.0 -0.4  -0.1   0.3\nlong   -0.5  1.0    0.1   0.3 -0.5 -0.5  0.0  0.3   0.2  -0.4\nweight -0.2  0.1    1.0   0.1  0.1 -0.3  0.8  0.5   0.6   0.3\nhighj  -0.1  0.3    0.1   1.0 -0.1 -0.3  0.1  0.2   0.1  -0.1\nm400    0.6 -0.5    0.1  -0.1  1.0  0.5  0.1 -0.3   0.1   0.6\nm110    0.6 -0.5   -0.3  -0.3  0.5  1.0 -0.1 -0.5  -0.1   0.1\ndisc    0.0  0.0    0.8   0.1  0.1 -0.1  1.0  0.3   0.4   0.4\npole   -0.4  0.3    0.5   0.2 -0.3 -0.5  0.3  1.0   0.3   0.0\njavel  -0.1  0.2    0.6   0.1  0.1 -0.1  0.4  0.3   1.0   0.1\nm1500   0.3 -0.4    0.3  -0.1  0.6  0.1  0.4  0.0   0.1   1.0\n\npca.ath = dudi.pca(athletes, scannf = FALSE)\npca.ath$eig\n\n [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n [8] 0.3067981 0.2669494 0.1018542\n\nfviz_eig(pca.ath, geom = \"bar\", bar_width = 0.3) + ggtitle(\"\")\n\n\n\n\n\n\n\nfviz_pca_var(pca.ath, col.circle = \"black\") + ggtitle(\"\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\nathletes[, c(1, 5, 6, 10)] = -athletes[, c(1, 5, 6, 10)]\ncor(athletes) |&gt; round(1)\n\n       m100 long weight highj m400 m110 disc pole javel m1500\nm100    1.0  0.5    0.2   0.1  0.6  0.6  0.0  0.4   0.1   0.3\nlong    0.5  1.0    0.1   0.3  0.5  0.5  0.0  0.3   0.2   0.4\nweight  0.2  0.1    1.0   0.1 -0.1  0.3  0.8  0.5   0.6  -0.3\nhighj   0.1  0.3    0.1   1.0  0.1  0.3  0.1  0.2   0.1   0.1\nm400    0.6  0.5   -0.1   0.1  1.0  0.5 -0.1  0.3  -0.1   0.6\nm110    0.6  0.5    0.3   0.3  0.5  1.0  0.1  0.5   0.1   0.1\ndisc    0.0  0.0    0.8   0.1 -0.1  0.1  1.0  0.3   0.4  -0.4\npole    0.4  0.3    0.5   0.2  0.3  0.5  0.3  1.0   0.3   0.0\njavel   0.1  0.2    0.6   0.1 -0.1  0.1  0.4  0.3   1.0  -0.1\nm1500   0.3  0.4   -0.3   0.1  0.6  0.1 -0.4  0.0  -0.1   1.0\n\npcan.ath = dudi.pca(athletes, nf = 2, scannf = FALSE)\npcan.ath$eig\n\n [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n [8] 0.3067981 0.2669494 0.1018542\n\nfviz_pca_var(pcan.ath, col.circle=\"black\") + ggtitle(\"\")\n\n\n\n\n\n\n\nfviz_pca_ind(pcan.ath) + ggtitle(\"\") + ylim(c(-2.5,5.7))\n\n\n\n\n\n\n\ndata(\"olympic\", package = \"ade4\")\nolympic$score\n\n [1] 8488 8399 8328 8306 8286 8272 8216 8189 8180 8167 8143 8114 8093 8083 8036\n[16] 8021 7869 7860 7859 7781 7753 7745 7743 7623 7579 7517 7505 7422 7310 7237\n[31] 7231 7016 6907\n\np = ggplot(tibble(pc1 = pcan.ath$li[, 1], score = olympic$score, id = rownames(athletes)),\n   aes(x = score, y = pc1, label = id)) + geom_text()\np + stat_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\nload(\"../data/screep7.RData\")\npcaS7 = dudi.pca(screep7, scannf = FALSE)\nfviz_eig(pcaS7,geom=\"bar\",bar_width=0.5) + ggtitle(\"\")\n\n\n\n\n\n\n\n#problem with dudi and prcomp eigenvalues\n#prcomp does not scale by default, dudi.pca does\n#fviz_eig(pcaS7,geom=\"bar\",width=0.3)\n#p7=prcomp(screep7,scale= TRUE)\n#p7$sdev^2\n#plot(p7)\n\npcaMsig3 = dudi.pca(Msig3transp, center = TRUE, scale = TRUE,\n                    scannf = FALSE, nf = 4)\nfviz_screeplot(pcaMsig3) + ggtitle(\"\")\n\nWarning in geom_bar(stat = \"identity\", fill = barfill, color = barcolor, :\nIgnoring empty aesthetic: `width`.\n\n\n\n\n\n\n\n\nids = rownames(Msig3transp)\ncelltypes = factor(substr(ids, 7, 9))\nstatus = factor(substr(ids, 1, 3))\ntable(celltypes)\n\ncelltypes\nEFF MEM NAI \n 10   9  11 \n\ncbind(pcaMsig3$li, tibble(Cluster = celltypes, sample = ids)) |&gt;\nggplot(aes(x = Axis1, y = Axis2)) +\n  geom_point(aes(color = Cluster), size = 5) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2) +\n  scale_color_discrete(name = \"Cluster\") + coord_fixed()\n\n\n\n\n\n\n\n## # For the record:\n## library(\"xcms\")\n## cdfpath = system.file(\"cdf\", package = \"faahKO\")\n## cdffiles = list.files(cdfpath, recursive = TRUE, full = TRUE)\n## xset = xcmsSet(cdffiles)\n## xset2 = group(xset)\n## xset2 = retcor(xset2)\n## xset2 = group(xset2, bw = 10)\n## xset3 = fillPeaks(xset2)\n## gt = groups(xset3)\n## mat1 = groupval(xset3, value = \"into\")\n\nload(\"../data/mat1xcms.RData\")\ndim(mat1)\n\n[1] 399  12\n\npcamat1 = dudi.pca(t(mat1), scannf = FALSE, nf = 3)\nfviz_eig(pcamat1, geom = \"bar\", bar_width = 0.7) + ggtitle(\"\")\n\n\n\n\n\n\n\ndfmat1 = cbind(pcamat1$li, tibble(\n    label = rownames(pcamat1$li),\n    number = substr(label, 3, 4),\n    type = factor(substr(label, 1, 2))))\npcsplot = ggplot(dfmat1,\n  aes(x=Axis1, y=Axis2, label=label, group=number, colour=type)) +\n geom_text(size = 4, vjust = -0.5)+ geom_point(size = 3)+ylim(c(-18,19))\npcsplot + geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2)\n\n\n\n\n\n\n\npcsplot + geom_line(colour = \"red\")\n\n\n\n\n\n\n\nlibrary(\"pheatmap\")\nload(\"../data/wine.RData\")\nload(\"../data/wineClass.RData\")\nwine[1:2, 1:7]\n\n  Alcohol MalicAcid  Ash AlcAsh  Mg Phenols Flav\n1   14.23      1.71 2.43   15.6 127    2.80 3.06\n2   13.20      1.78 2.14   11.2 100    2.65 2.76\n\npheatmap(1 - cor(wine), treeheight_row = 0.2)\n\n\n\n\n\n\n\nwinePCAd = dudi.pca(wine, scannf=FALSE)\ntable(wine.class)\n\nwine.class\n    barolo grignolino    barbera \n        59         71         48 \n\nfviz_pca_biplot(winePCAd, geom = \"point\", habillage = wine.class,\n   col.var = \"violet\", addEllipses = TRUE, ellipse.level = 0.69) +\n   ggtitle(\"\") + coord_fixed()\n\n\n\n\n\n\n\ndata(\"x\", package = \"Hiiragi2013\")\nxwt = x[, x$genotype == \"WT\"]\nsel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]\nxwt = xwt[sel, ]\ntab = table(xwt$sampleGroup)\ntab\n\n\n     E3.25 E3.5 (EPI)  E3.5 (PE) E4.5 (EPI)  E4.5 (PE) \n        36         11         11          4          4 \n\nxwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])\npcaMouse = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),\n  row.w = xwt$weight,\n  center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)\nfviz_eig(pcaMouse) + ggtitle(\"\")\n\nWarning in geom_bar(stat = \"identity\", fill = barfill, color = barcolor, :\nIgnoring empty aesthetic: `width`.\n\n\n\n\n\n\n\n\n## fviz_pca_ind(pcaMouse, geom = \"point\", col.ind = xwt$sampleGroup) +\n##   ggtitle(\"\") + coord_fixed()\n\n# TODO: duplicate of the above to avoid overlap; \n# can be removed once Quarto resolves this\nfviz_pca_ind(pcaMouse, geom = \"point\", col.ind = xwt$sampleGroup) +\n  ggtitle(\"\") + coord_fixed()\n\n\n\n\n\n\n\nu = seq(2, 30, by = 2)\nv = seq(3, 12, by = 3)\nX1 = u %*% t(v)\n\nMaterr = matrix(rnorm(60,1),nrow=15,ncol=4)\nX = X1+Materr\n\nouter(u, v)\n\n      [,1] [,2] [,3] [,4]\n [1,]    6   12   18   24\n [2,]   12   24   36   48\n [3,]   18   36   54   72\n [4,]   24   48   72   96\n [5,]   30   60   90  120\n [6,]   36   72  108  144\n [7,]   42   84  126  168\n [8,]   48   96  144  192\n [9,]   54  108  162  216\n[10,]   60  120  180  240\n[11,]   66  132  198  264\n[12,]   72  144  216  288\n[13,]   78  156  234  312\n[14,]   84  168  252  336\n[15,]   90  180  270  360\n\nggplot(data=data.frame(X), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()\n\n\n\n\n\n\n\nn = 100\np = 4\nY2 = outer(rnorm(n), rnorm(p)) + outer(rnorm(n), rnorm(p))\nhead(Y2)\n\n           [,1]       [,2]       [,3]       [,4]\n[1,] -0.1015264 -1.2018744 -0.1195410 -1.2109954\n[2,]  0.6183979  1.4116642  1.4748156 -0.3385532\n[3,] -0.6578056 -1.2090886 -1.6057652  0.7420605\n[4,] -0.4672680 -1.0598776 -1.1152444  0.2646805\n[5,] -0.1821787  0.1713696 -0.5086848  0.8664406\n[6,] -0.5857749 -2.8573348 -1.2049193 -1.6640022\n\nggplot(data=data.frame(Y2), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()\n\n\n\n\n\n\n\nsvd(Y2)$d # two non-zero eigenvalues\n\n[1] 2.577205e+01 1.507554e+01 2.457181e-15 5.708087e-16\n\nY = Y2 + matrix(rnorm(n*p, sd=0.01),n,p) # add some noise to Y2\nsvd(Y)$d # four non-zero eigenvalues (but only 2 big ones)\n\n[1] 25.77159150 15.08360530  0.10578758  0.09180731\n\nlibrary(\"MASS\")\nmu1 = 1; mu2 = 2; s1=2.5; s2=0.8; rho=0.9;\nsigma = matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2)\nsim2d = data.frame(mvrnorm(50, mu = c(mu1,mu2), Sigma = sigma))\nsvd(scale(sim2d))$d\n\n[1] 9.594338 2.438993\n\nsvd(scale(sim2d))$v[,1]\n\n[1] 0.7071068 0.7071068\n\nrespc = princomp(sim2d)\ndfpc  = data.frame(pc1=respc$scores[,1], \n                   pc2=respc$scores[,2])\n\nggplot(data.frame(sim2d), aes(x=X1,y=X2)) + geom_point()\n\n\n\n\n\n\n\nggplot(dfpc, aes(x=pc1, y=pc2)) + geom_point() + coord_fixed(2)\n\n\n\n\n\n\n\n## require(vcd)\n## uvec0=c(1,5.2,0.5,3.6)\n## n=length(uvec0)-1\n## vvec0=c(1,1.5,1.8,2.5,1.4)\n## p=length(vvec0)-1\n## rankone=function(uvec=uvec0,vvec=vvec0,factr=100){\n## Xout=uvec%*%t(vvec)*factr\n## n=length(uvec)\n## p=length(vvec)\n## dimnames(Xout)=list(U=c(\" \",paste(\"u\",1:(n-1),sep=\"\")),V=c(\" \",paste(\"v\",1:(p-1),sep=\"\")))\n## return(Xout)\n## }\n## \n## Xmat=rankone()\n## \n## makelabels=function(Xout=Xmat,step=0){\n## tab=as.table(Xout)\n## n=nrow(Xout)\n## p=ncol(Xout)\n## tab[1,1]=NA\n## if (step==0){\n##   tab[1,]=rep(NA,p)\n## tab[,1]=rep(NA,n)\n##   }\n##   return(tab)\n## }\n## \n## tabf0=makelabels(Xmat,step=0)\n## tabf1=makelabels(Xmat,step=1)\n## Xsub=Xmat[-1,-1]\n## dimnames(Xsub)=NULL\n## rowns=paste(\"$x_{\",1:n,\".}$\",sep=\"\")\n## colns=paste(\"$x_{.\",1:p,\"}$\",sep=\"\")\n## dimnames(Xsub)=list(rowns,colns)\n## require(xtable)\n## xtab=xtable(Xsub,floating=FALSE,digits=0)\n## xtab\n\n## np = prod(dim(Xmat))\n## rainbow_colors = colorspace::rainbow_hcl(np, c = 50, l = 70, start = 30, end = 360*(np-1)/np)\n## mycols = rainbow_colors[1:np]\n## mycols[c(1:(n+1), seq(n+2,(n+1)*(p+1),n+1))] = \"white\"\n\n## Xmat0=Xmat\n## #dimnames(Xmat0)=NULL\n## mosaic(Xmat,pop=FALSE,  gp = gpar(fill=matrix(mycols,ncol=p+1),fontsize=14))\n## #tabf0\n## #tabf1\n## labeling_cells(text = tabf0,clip=FALSE,gp_labels = gpar(fontsize = 14))(tabf0)\n## #labeling_border(labels =FALSE)\n\n## tabf2=tabf1\n## tabf2[-1,1]=0.1*tabf1[-1,1]\n## tabf2[1,-1]=0.1*tabf1[1,-1]\n## mosaic(Xmat,pop=FALSE,  gp = gpar(fill=matrix(mycols,ncol=p+1),fontsize=14))\n## labeling_cells(text = tabf2,clip=FALSE,gp_labels = gpar(fontsize = 14))(tabf2)\n\n## tabf3=tabf1\n## tabf3[-1,1]=0.05*tabf1[-1,1]\n## tabf3[1,-1]=0.2*tabf1[1,-1]\n## mosaic(Xmat,pop=FALSE,  gp = gpar(fill=matrix(mycols,ncol=p+1),fontsize=14))\n## labeling_cells(text = tabf3,clip=FALSE,gp_labels = gpar(fontsize = 14))(tabf3)\n\n## USV=svd(Xsub)\n## tabusv=tabf0\n## tabusv[1,]=c(round(USV$d[1],1),round(USV$v[,1],1))\n## tabusv[-1,1]=round(USV$u[,1],1)\n## #dimnames(tabusv)[[1]][1]=\"s1\"\n## mosaic(Xmat,pop=FALSE,  gp = gpar(fill=matrix(mycols,ncol=5),fontsize = 14))\n## labeling_cells(text = tabusv,clip=FALSE,gp_labels = gpar(fontsize = 14))(tabusv)\n\n## u2=c(5,2,3,4,1.5)\n## v2=c(2,7,3,4)\n## u3=c(0.5,1,2,2,3)\n## v3=c(1,0,4,2)\n## Xmat2=rankone(c(1,u2),c(1,v2),1)\n## Xmat3=rankone(c(1,u3),c(1,v3),5)\n## Xmat4=Xmat2+Xmat3\n## tab3f0=makelabels(Xmat4,step=0)\n## n=5;p=4;\n## mycols[c(1:(n+1),seq(n+2,(n+1)*(p+1),n+1))]=\"white\"\n## mosaic(Xmat4,pop=FALSE,  gp = gpar(fill=matrix(mycols,ncol=p+1),fontsize=14))\n## labeling_cells(text = tab3f0,clip=FALSE,gp_labels = gpar(fontsize = 14))(tab3f0)\n\n## tab3f1=makelabels(Xmat3,step=1)\n## mosaic(Xmat3,pop=FALSE,  gp = gpar(fill=matrix(mycols,ncol=p+1),fontsize=14))\n## labeling_cells(text = tab3f1,clip=FALSE,gp_labels = gpar(fontsize = 14))(tab3f1)\n\n## tab2f1=makelabels(Xmat2,step=1)\n## mosaic(Xmat2,pop=FALSE,  gp = gpar(fill=matrix(mycols,ncol=p+1),fontsize=14))\n## labeling_cells(text = tab2f1,clip=FALSE,gp_labels = gpar(fontsize = 14))(tab2f1)\n\n## Xsub4=Xmat4[-1,-1]\n## Xsub3=Xmat3[-1,-1]\n## Xsub2=Xmat2[-1,-1]\n## svd(Xsub3)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Grokking PCA",
    "section": "",
    "text": "Learning Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) based on Chapter 7 of Modern Statistics for Modern Biology."
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "Grokking PCA",
    "section": "Notebooks",
    "text": "Notebooks\n\nChapter 7: Multivariate Analysis - PCA and SVD fundamentals"
  }
]